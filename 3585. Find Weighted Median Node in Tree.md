Problem restatement & what to compute

You have a weighted tree (undirected, with n nodes numbered 0..n-1, edges with weights). For each query (u, v), consider the unique path between u and v. Let total = sum of weights along that path. The â€œweighted median nodeâ€ is defined as the first node x along the path from u â†’ v (starting at u) such that the sum of weights from u to x is â‰¥ total/2. Return that node index.

Because there can be up to ~10âµ nodes and ~10âµ queries, doing a naive path-reconstruction for each query (like DFS/BFS per query) will be too slow in worst case. 
Hello, World! System Design Newsletter
+1

ğŸ’¡ Efficient approach idea: Preprocessing + LCA + binary lifting + prefix-sum distances on tree

Main idea (common from blog solutions) 
CSDN Blog
+1
:

Root the tree at node 0 (given). Build an adjacency list.

Precompute (via a DFS) for each node:

parent[k][v]: the 2^k-th ancestor of v, for binary lifting (for LCA / jumping).

distToRoot[v]: the total weight sum from root (0) to v.

For each query (u, v):

Compute their LCA p = LCA(u, v).

The distance (total weight) along path u â†’ v is
D = distToRoot[u] + distToRoot[v] - 2 * distToRoot[p].

We want to walk from u toward v, and find the first node x along that path such that
distance from u to x â‰¥ D/2.

To locate x, we can do something like: binary-jump from u, checking mid-points, to find the proper ancestor/descendant along path to v. Because we know distances from root, we can compute the distance from u to any ancestor/descendant quickly.

This gives each query in 
ğ‘‚
(
log
â¡
ğ‘›
)
O(logn) time (for LCA + binary jumps), with preprocessing in 
ğ‘‚
(
ğ‘›
log
â¡
ğ‘›
)
O(nlogn). Total ~
ğ‘‚
(
(
ğ‘›
+
ğ‘„
)
log
â¡
ğ‘›
)
O((n+Q)logn), which is fine for 
ğ‘›
,
ğ‘„
â‰¤
10
5
n,Qâ‰¤10
5
. 
CSDN Blog
+1

ğŸ§‘â€ğŸ’» Java Code Implementation
import java.util.*;

public class Solution {


    int N;
    List<int[]>[] adj;
    int LOG;
    int[][] parent;   // parent[k][v] = 2^k-th ancestor of v
    long[] dist;      // dist[v] = distance (sum of weights) from root to v
    int[] depth;

    public int[] weightedMedian(int n, int[][] edges, int[][] queries) {
        N = n;
        adj = new List[n];
        for (int i = 0; i < n; i++) adj[i] = new ArrayList<>();
        for (int[] e : edges) {
            int u = e[0], v = e[1];
            int w = e[2];
            adj[u].add(new int[]{v, w});
            adj[v].add(new int[]{u, w});
        }

        LOG = 1;
        while ((1 << LOG) <= n) LOG++;
        parent = new int[LOG][n];
        dist = new long[n];
        depth = new int[n];

        // Precompute parent & dist via DFS from root = 0
        dfs(0, -1, 0, 0L);

        // Build binary-lifting table
        for (int k = 1; k < LOG; k++) {
            for (int v = 0; v < n; v++) {
                int mid = parent[k-1][v];
                parent[k][v] = (mid < 0 ? -1 : parent[k-1][mid]);
            }
        }

        int Q = queries.length;
        int[] ans = new int[Q];
        for (int i = 0; i < Q; i++) {
            int u = queries[i][0];
            int v = queries[i][1];
            ans[i] = getMedianOnPath(u, v);
        }
        return ans;
    }

    private void dfs(int v, int p, int d, long acc) {
        parent[0][v] = p;
        depth[v] = d;
        dist[v] = acc;
        for (int[] nb : adj[v]) {
            int nxt = nb[0], w = nb[1];
            if (nxt == p) continue;
            dfs(nxt, v, d + 1, acc + w);
        }
    }

    private int lca(int u, int v) {
        if (depth[u] < depth[v]) {
            int tmp = u; u = v; v = tmp;
        }
        // lift u up to same depth as v
        int diff = depth[u] - depth[v];
        for (int k = 0; k < LOG; k++) {
            if (((diff >> k) & 1) != 0) {
                u = parent[k][u];
            }
        }
        if (u == v) return u;
        for (int k = LOG - 1; k >= 0; k--) {
            if (parent[k][u] != parent[k][v]) {
                u = parent[k][u];
                v = parent[k][v];
            }
        }
        return parent[0][u];
    }

    private int getMedianOnPath(int u, int v) {
        int p = lca(u, v);
        long total = dist[u] + dist[v] - 2 * dist[p];
        long need = (total + 1) / 2;  // weight threshold (â‰¥ half), rounding up is safe

        // We'll move from u toward v. Idea: binary-jump from u upward/downward?
        // But path goes possibly up u->...->p then down p->...->v.
        // Simpler: Walk from u toward p, check if median lies on u->p segment. Else, walk downward from p toward v.

        // First check ascending part (u -> p)
        long upDist = dist[u] - dist[p];
        if (upDist >= need) {
            // median lies on u -> p upward path
            return jumpUp(u, need);
        } else {
            // then median lies on path from p down to v
            long remaining = need - upDist;
            // we need to go down from p toward v by distance = remaining
            return jumpDown(p, v, remaining);
        }
    }

    // jump up from u toward ancestor by exact weight â‰¥ w: return first node along that path satisfying dist[u]->node >= w
    private int jumpUp(int u, long w) {
        int curr = u;
        for (int k = LOG - 1; k >= 0; k--) {
            int anc = parent[k][curr];
            if (anc < 0) continue;
            long travelled = dist[u] - dist[anc];
            if (travelled < w) {
                curr = anc;
            }
        }
        return curr;
    }

    // Jump down from ancestor `anc` toward descendant `v`, by walking toward v, find first node at or beyond distance `downW` from anc.
    // Simplest: walk step by step using parent pointers reversed is complicated â€” you may need heavy-light decomposition or store children pointers.
    // For simplicity (and clarity), you could store along with parent also depth + parent pointers etc; or do a separate DFS from p to v, tracking the path and weights, similar to simple path reconstruction.
    // Given constraints, this may still pass if path lengths are not too big â€” but optimal solution uses more complex data structure.
    throw new UnsupportedOperationException("jumpDown not implemented â€” need extra data structures");
}


Note: The above sketch shows the main idea. The tricky part is jumpDown(â€¦): jumping downward from the LCA toward v by a certain weight distance, and finding the first node whose distance from p â‰¥ needed. To implement that efficiently you may need additional data structures (e.g. store Euler / heavy-light decomposition + segment tree / binary-search on path, or store parent & children + depth + maybe heavy-light + prefixâ€“sum on heavy paths). Many working solutions online use a variant of binary lifting that allows â€œjump k edgesâ€ rather than â€œjump k in weightâ€, but with edges of arbitrary weights, the weight-based â€œhalf-distanceâ€ doesnâ€™t map cleanly to a fixed number of jumps â€” hence a â€œwalk path + accumulate weightâ€ approach is more straightforward, but might degrade to 
ğ‘‚
(
ğ‘›
)
O(n) per query in worst case path length.

Because of this complexity, some solutions (for contest) use simple path reconstruction per query + linear scan when constraints permit â€” but that may get TLE for worst-case. 
Hello, World! System Design Newsletter
+1

ğŸ“Œ Simpler but slower approach (for easier implementation)

If constraints are smaller or you want simpler but possibly slower code: For each query:

Do a DFS / BFS from u to v to find the unique path (track parent map).

Reconstruct the path (list of nodes).

Compute the total weight along that path, then walk from u along that path accumulating weight until â‰¥ total/2 â€” return that node.

This is easy to code but runs in 
ğ‘‚
(
ğ‘
)
O(N) per query â€” OK only for small n and Q. 
Hello, World! System Design Newsletter
+1

âš ï¸ Edge-cases / Challenges

Because edge weights can be large (up to 10â¹), do all weight sums in long.

Path may go up (from u up to LCA) and then down (from LCA to v), so median may be on either segment. Handling the â€œdownwardâ€ part efficiently (without reconstructing full path) is tricky.

If you donâ€™t pre-process carefully (e.g. with heavy-light decomposition), worst-case per query time may degrade.
